\documentclass{article}

% required includes
\usepackage{graphicx}
\usepackage{url}

\usepackage{tikz}
\usetikzlibrary{shapes,arrows}
% styles for flowcharts
\tikzstyle{machine} = [star, star points=7, star point ratio=0.8, draw, text width=5em, text badly centered, node distance=3cm, inner sep=0pt]
\tikzstyle{data} = [trapezium, trapezium left angle=70, trapezium right angle=-70, draw,text centered]
\tikzstyle{process} = [rectangle, draw, text width=5em, text centered, rounded corners, minimum height=4em] 

\addtolength{\textwidth}{10pt}

\title{CAT-DWAM, A Question Answering System}
\author{
Alec Story,
Ansu Abraham,
Craig Frey,\\
Dustin Tiedemann,
Michael Zhu,
Thomas Levine,
Whitney Foster\\
}

\begin{document}
\maketitle

\section{Overview}

% What are you doing. Why are you doing it.
% ``this is effectively a statement of your hypothesis, so you should
% include justification of why you think that the particular approach
% will be effective

\section{The QA System}
% we need:
% a detailed walkthrough of what our system did to handle one question
% (in general) in the corpus. include the system output (enough to
% convince them that it is actually doing what we say)

% this is pretty much the final system proposal from part
% one. delete/rewrite if necessary (we need a description of the
% baseline in our final report)
We implemented our question answering system in Python, and use external
tools such as the natural language toolkit.

Our design mimics the design of Watson, with the intention of trying out a
variety of techniques to select answers from documents, and to use a machine
learning system to choose the appropriate one.  See figure \ref{diagram} for
a graphical overview.

We first shred the documents for each question into all of the component verb
and noun phrases.  We pass these candidates into a battery of evaluators that
assign a confidence value to each of them, and pass these confidences into a
machine learning algorithm that combines them to produce a ranked output.

From this ranked output we choose our final answers.

\begin{figure}
 \label{diagram}
 \begin{center}
  \begin{tikzpicture}[node distance=2.5cm, auto, >=stealth]
   % nodes
   \node[data] (Q)                                      {Question};
   \node[process] (ch) [below of=Q]                     {Chunker};
   \node[data] (docs) [right of=ch]                     {Documents};
   \node[process] (a2) [below of=ch]                    {Answer Evaluator};
   \node[process] (a1) [left of=a2]                     {Answer Evaluator};
   \node[process] (a3) [right of=a2]                    {Answer Evaluator};
   \node[machine] (as) [below of=a2]                    {Score aggregator};
   \node[data] (A) [below of=as]                        {Answer};
   
   % edges
   	\draw[->] (Q) to (ch);
	\draw[->] (docs) to (ch);
	\draw[->] (ch.south) to [out=270, in=90] (a1);
	\draw[->] (ch.south) to [out=270, in=90] (a2);
	\draw[->] (ch.south) to [out=270, in=90] (a3);
	\draw[->] (Q.west) to [out=180, in=180] (a1);
	\draw[->] (Q.west) to [out=180, in=180] (a2);
	\draw[->] (Q.west) to [out=180, in=180] (a3);
	\draw[->] (a1.south) to [out=270, in=135] (as);
	\draw[->] (a2.south) to [out=270, in=90] (as);
	\draw[->] (a3.south) to [out=270, in=45] (as);
	\draw[->] (as) to (A);
  \end{tikzpicture}
  \caption{Data Flow}
  \label{flowchart}
 \end{center}
\end{figure}

\subsection{Parsing the Corpus Documents}
% Talk about Whitney's Shredder module here

\subsection{Question Evaluators}

\subsubsection{Question-Document Comparison}

	We implemented a system to compare the question and answer against the
	proposed document in two ways:  one, with a na\"ive diffing algorithm that
	simply seeks out the longest indentical sequence, and one by implementing
	the Smith-Waterman local alignment
	algorithm.\footnote{\url{http://en.wikipedia.org/wiki/Smith-Waterman_algorithm}}
	We also implemented a system to attempt to re-write questions, discussed
	below, and applied these same techniques to the re-written question

	The na\"ive method does an adequate job when the document highly resembles
	the question, but does very poorly otherwise because it has no tolerance of
	faults.  The local alignment does much better for documents that resemble
	the question a great deal, but are not precise - it can smoothly deal with
	small differences, and still produce a good alignment.  This is a standard
	technique from bioinformatics, and is used extensively to align small
	sequences of DNA, RNA or protein against a larger corpus, such as a
	chromosome or a whole genome.  This is a fairly similar usage to aligning a
	question against a document which is much longer than the question.

	We chose Smith-Waterman over the similar Needleman-Wunsch, of which is a
	special case, because Needleman-Wunsch creates a \emph{global} alignment,
	which forces all of the characters in both strings to align.  This leads to
	the insertion of many gaps, and needlessly aligns words which are useless,
	for example, ``Who is Queen of England?'' should align well against
	``Elizabeth is Queen of England'' and it does, under Smith-Waterman, but the
	``who'' causes Needleman-Wunsch to choke.  Similarly, Needleman-Wunsch does
	not excuse one string for being shorter than the other, and chokes on all of
	the apparent gaps that it needs to insert, so it is not a good choice.

	We locate what we best believe to be the question via these two methods, and
	then evaluate how far away from that segment of the document the answer is.
	If it is fairly close, that is a good indicator that it is highly related to
	the question, but if it is far, it is likely to be unrelated.  We output
	that distance, and either the length of the na\"ive alignment or the score
	that Smith-Waterman produces (higher is better for both) because very short
	or very poor alignments are less likely to reflect an actual representation
	of the question.

\subsubsection{Punctuation}

	1 if the candidate answer is immmediately followed by a comma, period,
	quotation marks, semicolon, or exclamation mark, and 0 otherwise. This is a
	good heuristic by which to rank candidate answers since it retrieves a
	property from the context where the answer appears in the document.

\subsubsection{Apposition Filter}
% Alec

	Because we thought it would be difficult to analyze complete parse
	information for questions, we decided to tackle apposition by a simple
	heuristic:  does "question, answer," align better than "question answer"?
	If it does, then the answer is likely to be in apposition.  We also applied
	this to re-written questions.


\subsubsection{Number and Quantity Answer Type Filters}
% Tom

\subsubsection{Measurement Units or Matching Type Filters}
% Tom

\subsubsection{Sequence Length}
% Mike

\subsubsection{Word Vector Comparison}

This evaluation measures the angle difference between the word vector of the
question and the answer, after removing stop words, and scaled to [0,1].

\subsubsection{Bag of Words Filter}

Simply how many times the words in the question (without stop words) appear in
the answer, a simpler but less precise metric than the word vector.

\subsubsection{Novelty Factor}

Novelty is whether at least one non-stop word in the answer that is not found in
the question.  This is intuitive because an answer that is completely non-novel
is unlikely to be interesting.

We implemented this in two slightly different ways:  one, that just reports the
presence or absence of novel words, and one that counts the number of novel
words in the answer.  We split them this way to make it clear to the learning
algorithm that any novelty is very important, but extra novelty probably doesn't
matter much.

\subsubsection{Part of Speech Matching}
% Craig

\subsection{Score Aggregation}
We used supervised classification algorithms to aggregate the scores produced
by the separate answer evaluators. We trained the classifiers to predict whether
a candidate answer was correct based on the scores produced by the answer evaluators.

Each of the answer evaluators produced at least one numerical for each answer
candidate; we used these as predictors for the classification algorithms.
If we thought that some of these predictors might interact, we used their products
as predictors as well.

For each candidate answer, we checked whether that candidate was a correct answer based on
the regular expressions. We used the correctness of the answer (correct or incorrect)
as the response. We were thus able to predict whether an answer was correct
based on the scores from the answer evaluators.

The classifiers that we used were
\begin{itemize}
\item support vector machines (SVM),
\item Fisher Discriminant Analysis (FDA),
\item Spectral Regression Discriminant Analysis (SRDA) and
\item Penalized Discriminant Analysis (PDA)
\end{itemize}
%We tried these classifiers separately rather than 
\section{Results}

% we need:
% the output file of answers produced by our system for the questions
% from the development corpus
% an evaluation using the Mean Reciprocal Rank Evaulation Measure)
% an analysis of the system's performance on questions of the
% development corpus
   % what worked?
   % what didn't work?
   % which component was strongest/weakest?
   % how does our system compare to the baseline?
% responses from our system for test set released on Wednesday
   % we also need to submit this file

\subsection{Baseline Results}

\subsection{Graphical Methods}

\subsection{Best Run Results}

\section{Conclusion}

\end{document}tk.
